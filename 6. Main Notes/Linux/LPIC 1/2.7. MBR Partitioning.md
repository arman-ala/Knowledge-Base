# MBR Partitioning

2025-10-11 06:12
Status: #DONE 
Tags: [[Linux]]

---
# Comprehensive Guide to MBR Partitioning and Linux Naming Conventions

## MBR Partitioning Architecture

### Historical Partitioning Requirements

**Legacy Constraints:**
- Every hard disk required **at least one primary partition** for mounting due to BIOS limitations that could only recognize bootable code in primary partitions
- **Extended partitions** couldn't exist alone without primary partitions because the MBR structure required at least one primary partition to be bootable and to contain the partition table itself
- Boot requirements mandated primary partitions for operating systems because early BIOS systems could only load boot code from primary partitions
- Modern systems have relaxed these constraints with advanced bootloaders that can boot from logical partitions or even directly from filesystems

**Why These Constraints Existed:**
The MBR (Master Boot Record) was designed in the early 1980s when storage was measured in megabytes and systems were far simpler. The MBR structure occupies the first 512 bytes of a disk and contains:
- Boot code (446 bytes)
- Disk signature (4 bytes)
- MBR signature (2 bytes)
- Partition table (64 bytes)

With only 64 bytes for the partition table, and each partition entry requiring 16 bytes, the MBR could only support four primary partitions. This design was sufficient for early systems but became a significant limitation as storage needs grew.

**Problems with MBR Constraints:**
- **Limited Partition Count**: The 4-primary-partition limit became increasingly restrictive as users needed more operating systems or data separation
- **Boot Process Limitations**: Early BIOS systems could only boot from primary partitions, requiring careful planning for multi-boot setups
- **Storage Capacity Limits**: MBR uses 32-bit addressing for logical block addressing (LBA), limiting it to 2TB disks (2^32 × 512 bytes)
- **No Redundancy**: The MBR contains a single partition table with no built-in redundancy or error correction

**Positive Points of MBR:**
- **Universal Compatibility**: MBR was supported by virtually all operating systems and BIOS systems for decades
- **Simplicity**: The straightforward structure made it easy to implement and understand
- **Small Footprint**: The compact 512-byte structure left maximum space for data
- **Boot Flexibility**: Despite limitations, MBR allowed for multi-boot configurations through careful planning

### Partition Layout Flexibility

**Spatial Arrangement Options:**
```
VALID LAYOUT EXAMPLES:
[P][P][P][E]             # Traditional four-partition layout
[P][  Free Space ][P]    # Discontiguous with unused space
[P][E][L][L][ Free ]     # Mixed with logical partitions
[P][P][ Free ][E][L]     # Complex arrangement
[P]                      # One single partion for one hard disk
[E]                      # One single partion for one hard disk
```

![[2.7. Partitioning.png]]

**Technical Explanation:**
MBR partitions are defined by their starting cylinder, head, and sector (CHS) in older systems, or by logical block addressing (LBA) in modern systems. This allows partitions to be placed anywhere on the disk, not just consecutively. The partition table entries contain:
- Status byte (active/inactive)
- Starting CHS address (3 bytes)
- Partition type (1 byte)
- Ending CHS address (3 bytes)
- Starting LBA (4 bytes)
- Size in LBA (4 bytes)

**Why Flexibility Mattered:**
- **Multi-Boot Systems**: Different operating systems could be installed in non-adjacent partitions
- **Performance Optimization**: Frequently accessed partitions could be placed at the faster outer edge of the disk
- **Data Organization**: Related data could be grouped together even if not contiguous
- **Future Expansion**: Free space could be strategically placed for later partition creation

**Problems with MBR Flexibility:**
- **Fragmentation**: Non-contiguous free space could lead to inefficient use of disk space
- **Complex Management**: Planning partition layouts required expertise and foresight
- **Performance Impact**: Poorly placed partitions could result in slower access times
- **Recovery Complexity**: Fragmented partition layouts made data recovery more difficult

**Positive Points of MBR Flexibility:**
- **Adaptability**: Administrators could customize layouts to specific needs
- **Optimization Potential**: Knowledgeable users could optimize for performance
- **Gradual Expansion**: Additional partitions could be added as needed without reformatting
- **Multi-OS Support**: Different operating systems with different requirements could coexist

## Partition Management Limitations

### Static Partition Challenges

**Resizing Difficulties:**
```
BEFORE RESIZE:
[ Partition A ][ Unused Space ][ Partition B ]

AFTER RESIZE (Desired):
[      Expanded Partition A     ][ Partition B ]

REQUIRED PROCESS:
1. Delete Partition A (losing all data)
2. Create new larger Partition A
3. Restore data from backup
4. Repeat for adjacent partitions if needed
```

![[2.7. delete partition.png]]

**Technical Explanation:**
MBR partitions are defined by their starting and ending LBAs in the partition table. To resize a partition, these values must be updated. However, the filesystem within the partition must also be resized to match the new boundaries. This process is complicated by:

- **Filesystem Dependencies**: The filesystem has its own structures (superblocks, inodes, allocation tables) that assume a fixed size
- **Data Block Relocation**: Expanding a partition forward requires moving all subsequent partitions and their data
- **Boot Sector Relocation**: If the partition contains boot code, it must be updated to reflect the new size
- **Partition Table Updates**: The MBR itself must be modified with the new partition boundaries

**Why Resizing Was Difficult:**
- **No Native Support**: MBR was designed when disks were small and resizing was rarely needed
- **Data Integrity Risks**: Any error during the process could result in data loss
- **Complex Dependencies**: Filesystems, partition tables, and boot loaders all needed coordinated updates
- **Lack of Tools**: Early tools for resizing were primitive and unreliable

**Problems with Static Partitioning:**
- **Data Loss Risk**: The resize process required deleting and recreating partitions, risking data loss
- **Downtime**: Systems had to be taken offline for resizing operations
- **Complexity**: The process required multiple steps and careful planning
- **Backup Dependency**: Reliable backups were essential but not always available

**Positive Points (Leading to Innovation):**
- **Problem Identification**: These limitations clearly demonstrated the need for better solutions
- **Innovation Driver**: The static nature of MBR led to the development of LVM and GPT
- **Best Practices**: Established the importance of careful initial partition planning
- **Tool Development**: Spurred the creation of more sophisticated partition management tools

### Space Recovery Issues

- Deleting **non-adjacent** partitions creates **fragmented free space**
- **Unrecoverable gaps** occur between remaining partitions
- **Wasted capacity** when free space isn't contiguous
- **LVM (Logical Volume Manager)** solves these limitations in LPIC-2

**Technical Explanation:**
When partitions are deleted, the space they occupied becomes available, but it remains in its original location on the disk. If this space is not adjacent to another partition or free space, it becomes fragmented. For example:

```
Initial State:
[Partition A][Partition B][Partition C][Free Space]

After Deleting Partition B:
[Partition A][   Free Space   ][Partition C][Free Space]
```

Now there are two separate free spaces that cannot be combined into a single partition without moving Partition C.

**Why Fragmented Free Space Was Problematic:**
- **Inefficient Utilization**: Small, fragmented spaces couldn't be used for larger partitions
- **Management Complexity**: Administrators had to track multiple small free spaces
- **Performance Impact**: New partitions created in fragmented spaces might be scattered across the disk
- **Planning Challenges**: Future expansion became difficult to predict and implement

**Problems with Fragmented Free Space:**
- **Capacity Waste**: Small unusable spaces reduced effective storage capacity
- **Administrative Burden**: Required constant monitoring and management
- **Limitation on New Partitions**: Only partitions smaller than the largest contiguous free space could be created
- **Performance Degradation**: New partitions in fragmented areas might have poor performance

**Positive Points (Leading to Solutions):**
- **Clear Problem Statement**: Fragmented free space was an obvious limitation that needed addressing
- **LVM Development**: This issue directly led to the creation of Logical Volume Manager
- **GPT Advantages**: Modern GPT partitioning schemes handle free space more efficiently
- **Best Practices**: Established the importance of leaving contiguous free space for future expansion

## Linux Partition Naming Conventions

### IDE Drive Naming Scheme

**Device Naming Pattern:** `/dev/hd[a-d][1-4]`

**Breakdown:**
- **hd**: Identifies IDE/PATA drives, derived from "hard disk"
- **[a-d]**: Drive letter assignment
  - `a` = Primary channel, master
  - `b` = Primary channel, slave  
  - `c` = Secondary channel, master
  - `d` = Secondary channel, slave
- **[1-4]**: Partition numbers (1-4 reserved for primary/extended)

**IDE Channel Architecture:**
```
Primary IDE Channel:
├── Master: /dev/hda
└── Slave:  /dev/hdb

Secondary IDE Channel:
├── Master: /dev/hdc
└── Slave:  /dev/hdd
```

**Historical Context:**
IDE (Integrated Drive Electronics) was the dominant storage interface in consumer computers from the late 1980s through the mid-2000s. The naming convention reflected the physical architecture of IDE systems, which typically had two channels (primary and secondary), each supporting two devices (master and slave).

**Why IDE Used This Naming:**
- **Physical Mapping**: The naming directly corresponded to physical connections on the motherboard
- **Master/Slave Configuration**: Each channel had a master device (bootable) and a slave device
- **BIOS Limitations**: Early BIOS systems could only boot from primary master devices
- **Simplicity**: The straightforward naming made it easy to identify drives

**Problems with IDE Naming:**
- **Limited to 4 Drives**: The naming scheme only supported up to 4 IDE devices
- **Master/Slave Conflicts**: Incorrect jumper settings could cause drives not to be detected
- **Parallel Interface Limitations**: IDE used parallel signaling which became a bottleneck at higher speeds
- **Cable Length Restrictions**: IDE cables were limited to 18 inches, restricting physical layout
- **No Hot-Swapping**: IDE devices could not be added or removed while the system was running

**Positive Points of IDE:**
- **Cost-Effective**: IDE drives were inexpensive and widely available
- **Simple Installation**: The interface was straightforward for most users
- **Good Performance (for its time)**: IDE evolved to support transfer rates up to 133 MB/s
- **Universal Compatibility**: Virtually all motherboards included IDE controllers
- **Clear Naming Convention**: The /dev/hd* naming was intuitive and predictable

### SCSI/SATA/NVMe Drive Naming

**Device Naming Pattern:** `/dev/sd[a-z][1-4]`

**Breakdown:**
- **sd**: Identifies SCSI, SATA, USB, and NVMe drives, derived from "SCSI disk"
- **[a-z]**: Drive letter in detection order
  - `a` = First detected drive
  - `b` = Second detected drive
  - Continues through alphabet
- **[1-4]**: Partition numbers (1-4 reserved for primary/extended)

**Why the Transition from /dev/hd* to /dev/sd*:**
The transition reflected the move from IDE/PATA to newer storage technologies. SCSI (Small Computer System Interface) was originally a high-end alternative to IDE, but its naming convention was adopted for SATA (Serial ATA) because:

1. **SCSI Command Set**: SATA drives use the SCSI command set through translation (SAT)
2. **Unified Driver Model**: Linux implemented a unified SCSI layer that could handle both real SCSI and SATA devices
3. **Scalability**: The SCSI model supported more devices and more advanced features
4. **Future-Proofing**: The SCSI-based naming was more extensible for future storage technologies

**Problems with /dev/sd* Naming:**
- **Detection Order Dependency**: Device names can change between reboots if hardware configuration changes
- **Limited Alphabet**: Only 26 letters available for drives (a-z)
- **No Physical Mapping**: Names don't correspond to physical locations, making troubleshooting harder
- **Persistence Issues**: Scripts relying on device names might break after hardware changes

**Positive Points of /dev/sd* Naming:**
- **Scalability**: Supports more devices than IDE's 4-drive limit
- **Technology Agnostic**: Works with SCSI, SATA, SAS, USB, and even NVMe (through translation)
- **Standardized**: Consistent naming across different storage types
- **Extensible**: The model can accommodate new storage technologies
- **Simplified Driver Model**: Unified approach reduces kernel complexity

**NVMe Naming Evolution:**
For NVMe (Non-Volatile Memory Express) drives, Linux introduced a new naming pattern:
`/dev/nvme[controller]n[namespace]p[partition]`

This reflects NVMe's architecture where:
- Multiple controllers can exist
- Each controller can have multiple namespaces (logical storage units)
- Each namespace can have multiple partitions

This naming is more descriptive and stable than the /dev/sd* scheme, but /dev/sd* is still used for NVMe in some configurations for compatibility.

## Logical Partition Numbering System

### Automatic Numbering Scheme

**Partition Number Allocation:**
```
PRIMARY/EXTENDED: 1-4
LOGICAL PARTITIONS: 5-15
```

**Technical Explanation:**
The MBR partition table has only four entries (1-4), each 16 bytes in size. To support more than four partitions, one of these primary partitions can be designated as an "extended partition." The extended partition itself doesn't contain data but acts as a container for logical partitions.

Logical partitions are defined within the extended partition using a linked list of Extended Boot Records (EBRs). Each EBR contains:
- A partition table entry for the logical partition
- A pointer to the next EBR
- A partition table entry for the next logical partition (if any)

This linked list structure allows multiple logical partitions to exist within a single extended partition.

**Why the Numbering is Fixed:**
- **MBR Limitation**: The original MBR specification only defined four partition entries (1-4)
- **Extended Partition Innovation**: Logical partitions (5+) were added later as an extension to MBR
- **Backward Compatibility**: The numbering scheme maintained compatibility with existing tools and systems
- **Implementation Simplicity**: Fixed ranges made partition management code simpler

**Problems with Logical Partition Numbering:**
- **Limited to 15 Partitions**: Only 11 logical partitions (5-15) were supported
- **EBR Chain Vulnerability**: If one EBR in the chain was corrupted, all subsequent logical partitions could be lost
- **Tool Compatibility**: Some older tools didn't properly handle logical partitions beyond a certain number
- **Complex Recovery**: Recovering logical partitions was more complex than primary partitions

**Positive Points of Logical Partition Numbering:**
- **Predictable**: The numbering scheme was consistent and easy to understand
- **Extended MBR Functionality**: Allowed more than four partitions without changing the MBR specification
- **Widespread Support**: Virtually all partitioning tools supported this scheme
- **Practical Solution**: Solved the immediate problem of limited partitions for most users

### Practical Examples

**Scenario 1: Mixed Partition Types**
```
Disk: /dev/sda
├── /dev/sda1  Primary (boot)
├── /dev/sda2  Primary (root)
├── /dev/sda3  Primary (home)
├── /dev/sda4  Extended
│   ├── /dev/sda5  Logical (swap)
│   ├── /dev/sda6  Logical (var)
│   └── /dev/sda7  Logical (tmp)
└── Free space
```

**Analysis:**
This layout uses three primary partitions for critical system functions (boot, root, home) and one extended partition containing three logical partitions (swap, var, tmp). This is a common configuration for MBR systems that need more than four partitions.

**Scenario 2: Maximum Logical Partitions**
```
Disk: /dev/sdb
├── /dev/sdb1  Primary
├── /dev/sdb2  Primary  
├── /dev/sdb3  Primary
├── /dev/sdb4  Extended
│   ├── /dev/sdb5  to /dev/sdb15 (11 logical partitions)
└── No free space
```

**Analysis:**
This configuration maximizes the number of partitions under MBR by using three primary partitions and one extended partition containing 11 logical partitions. While theoretically possible, this approach is not recommended because:
- It creates a complex EBR chain that is vulnerable to corruption
- It leaves no free space for future expansion
- Managing 14 partitions becomes administratively complex
- Performance may suffer due to fragmentation

## Modern Partitioning Considerations

### Evolution from Traditional Constraints

**Current Flexibility:**
- GPT partitioning removes 4-primary partition limit
- UEFI systems don't require primary partitions for booting
- Advanced filesystems can span multiple partitions
- Containerization reduces dependency on physical partitioning

**GPT (GUID Partition Table) Advantages:**
- **More Partitions**: Supports up to 128 partitions by default
- **Larger Disks**: Uses 64-bit LBA addressing, supporting disks up to 8 ZB (zettabytes)
- **Partition Naming**: Uses GUIDs (Globally Unique Identifiers) for partition identification
- **Redundancy**: Primary GPT is at the beginning of the disk, with a backup GPT at the end
- **CRC32 Checksums**: Protects partition table integrity
- **UEFI Support**: Designed for UEFI systems, which don't have the same boot limitations as BIOS

**Why GPT Replaced MBR:**
- **Storage Capacity**: MBR's 2TB limit became problematic as drive sizes increased
- **Partition Count**: The 4-partition limit was increasingly restrictive
- **Data Integrity**: GPT's redundancy and checksums provide better protection
- **Modern Boot Requirements**: UEFI systems work better with GPT
- **Industry Standard**: GPT has been adopted as the standard for modern systems

**Problems with GPT:**
- **Compatibility Issues**: Older systems (pre-2010) may not support GPT
- **Complexity**: More complex structure than MBR
- **Tool Requirements**: Requires modern partitioning tools
- **BIOS Limitations**: Some BIOS systems have limited GPT support

**Positive Points of GPT:**
- **Future-Proof**: Designed to accommodate future storage technologies
- **Robust**: Redundancy and checksums protect against corruption
- **Flexible**: Supports many partitions and large disks
- **Standardized**: Adopted across the industry
- **Feature-Rich**: Supports features like partition names and attributes

### Best Practices Summary

**Partition Planning:**
- Reserve numbers 1-4 for primary/extended partitions
- Use logical partitions for additional storage needs
- Leave free space for future expansion when possible
- Consider GPT for systems requiring more than 15 partitions

**Modern Recommendations:**
- **For New Systems**: Use GPT unless compatibility with very old systems is required
- **For Large Storage**: GPT is essential for disks larger than 2TB
- **For Multi-Boot**: GPT provides better support for multiple operating systems
- **For Servers**: GPT's redundancy features provide better protection for critical systems

**Naming Convention Awareness:**
- **IDE Systems**: `/dev/hd[a-d][1-4]` and `/dev/hd[a-d][5-15]` (legacy)
- **Modern Systems**: `/dev/sd[a-z][1-4]` and `/dev/sd[a-z][5-15]`
- **NVMe Systems**: `/dev/nvme[controller]n[namespace]p[partition]` (newest)

**Persistent Naming:**
Modern Linux systems provide persistent naming mechanisms to address the detection order dependency:
- **by-id**: Uses unique hardware identifiers
- **by-path**: Uses physical connection paths
- **by-uuid**: Uses filesystem UUIDs
- **by-label**: Uses filesystem labels

This comprehensive understanding of MBR partitioning and Linux naming conventions provides the foundation for effective disk management in traditional systems, while highlighting the evolution toward more flexible modern partitioning schemes. The transition from MBR to GPT represents a significant improvement in storage technology, addressing the limitations of the past while providing a foundation for future storage needs.